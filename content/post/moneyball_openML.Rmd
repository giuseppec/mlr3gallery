---
title: mlr3 and OpenML - Moneyball use case
author: Philipp Kopper
date: '2020-03-21'
slug: mlr3-OpenML-Tuning
categories: []
tags: ['mlr3', 'OpenML', 'tuning']
packages: ['mlr3', 'mlr3learners', 'mlr3tuning']
---

This use case shows how to easily embed the rich data available via [OpenML](https://www.openml.org/) into our mlr3 workflow.
The idea is to show how to access a variety of real world data problems and how to tackle them via `mlr3`.
Furthermore, we show how a __good__ model can be found using `mlr3tuning`.
It assumes only little knowledge in ML or mlr3.
It requires no knowledge on OpenML or parameter tuning in mlr3.
Knowledge on missing value impuation is helpful but not necessary.
The [mlr3book](https://mlr3book.mlr-org.com/) is a detailed ressource of the methods and functions used here.

The following operations are shown:

* Creating tasks and learners
* Training and predicting
* Resampling / Cross-validation
* Tuning
* Benchmarking, to compare multiple learners (or specifications)

## Loading basic packages

```{r}
# tasks, train, predict, resample, benchmark
library("mlr3")
# about a dozen reasonable learners
library("mlr3learners")
# Tuning
library("mlr3tuning")
# lots of measures for evaluation
library("mlr3measures")
# Retreiving the data
library("OpenML")
```

## Retreiving the data from OpenML

Currently, we are still working on a generic embedding of `OpenML` into `mlr3`.
The use case will be updated as soon as we release the workflow.
However, we can still use the `OpenML` package to retrieve data (and more) straight away.
Typically, you can retrieve the data with an `id`.
The `id` can be found on [OpenML](https://www.openml.org/).
We choose the `41021`as our `oml_id`.
The related web page cann be accessed [here](https://www.openml.org/d/41021).
This data set is uploaded by [Joaquin Vanschoren](https://www.openml.org/u/2).

```{r}
oml_id = 41021

oml_dat = getOMLDataSet(data.id = oml_id)
```

The decription indicates that the data set is associated to baseball or more precisely the story of [Moneyball](https://www.imdb.com/title/tt1210166/).

```{r}
oml_dat
```

However, the description within the `OpenML` object is not very detailed.
The previously referenced [page](https://www.openml.org/d/41021) however states the following:

In the early 2000s, Billy Beane and Paul DePodesta worked for the Oakland Athletics. 
While there, they literally changed the game of baseball. 
They didn't do it using a bat or glove, and they certainly didn't do it by throwing money at the issue; in fact, money was the issue. 
They didn't have enough of it, but they were still expected to keep up with teams that had much deeper pockets. 
This is where Statistics came riding down the hillside on a white horse to save the day. 
This data set contains some of the information that was available to Beane and DePodesta in the early 2000s, and it can be used to better understand their methods.

This data set contains a set of variables that Beane and DePodesta focused heavily on. 
They determined that stats like on-base percentage (OBP) and slugging percentage (SLG) were very important when it came to scoring runs, however they were largely undervalued by most scouts at the time. 
This translated to a gold mine for Beane and DePodesta. 
Since these players weren't being looked at by other teams, they could recruit these players on a small budget. 
The variables are as follows:

* Team
* League
* Year
* Runs Scored (RS)
* Runs Allowed (RA)
* Wins (W)
* On-Base Percentage (OBP)
* Slugging Percentage (SLG)
* Batting Average (BA)
* Playoffs (binary)
* RankSeason
* RankPlayoffs
* Games Played (G)
* Opponent On-Base Percentage (OOBP)
* Opponent Slugging Percentage (OSLG)

If you are a baseball enthusiast these statistics seem very informative.
If basebakk

The summary shows how this data we are dealing with looks like:

```{r}
summary(oml_dat$data)
```

## Creating tasks and learners

Lets work on the canonical, simple iris data set, and try out some ML algorithms.
We will start by using a decision tree with default settings.

```{r}
# creates a mlr3 task from scratch, from a data.frame
# 'target' names the column in the dataset we want to learn to predict
task = TaskClassif$new(id = "iris", backend = iris, target = "Species")
# in this case we could also take the iris example from mlr3's dictionary of shipped example tasks
# 2 equivalent calls to create a task. The second is just sugar for the user.
task = mlr_tasks$get("iris")
task = tsk("iris")
print(task)
# create learner from dictionary of mlr3learners
# 2 equivalent calls:
learner1 = mlr_learners$get("classif.rpart")
learner1 = lrn("classif.rpart")
print(learner1)
```

## Train and predict

Lets do usual ML operations: Train on some obervations, predict on others.

```{r}
# train learner on subset of task
learner1$train(task, row_ids = 1:120)
# this is what the decision tree looks like
print(learner1$model)
# predict using observations from task
preds = learner1$predict(task, row_ids = 121:150)
# predict using "new" observations from an external data.frame
preds = learner1$predict_newdata(newdata = iris[121:150, ])
print(preds)
```

## Evaluation

Let's score our prediction object with some metrics.
And take a deeper look by inspecting the confusion matrix.

```{r}
head(as.data.table(mlr_measures))
s = preds$score(msr("classif.acc"))
print(s)
s = preds$score(msrs(c("classif.acc", "classif.ce")))
print(s)
cm = preds$confusion
print(cm)
```

## Changing hyperpars

The learner contains information about all parameters that can be configured, including data type, constraints, defaults, etc.
We can change the hyperparameters either during construction of later through an active binding.

```{r}
print(learner1$param_set)
learner2 = lrn("classif.rpart", predict_type = "prob", minsplit = 50)
learner2$param_set$values$minsplit = 50
```

## Resampling

Resampling simply repeats the train-predict-score loop and collects all results in a nice `data.table`.

```{r, size = "tiny"}
cv10 = rsmp("cv", folds = 10)
r = resample(task, learner1, cv10)
print(r)
r$score(msrs(c("classif.acc", "classif.ce")))
print(r$data)
# get all predictions nicely concatenated in a table
preds = r$prediction()
print(preds)
cm = preds$confusion
print(cm)
```

## Populating the learner dictionary

mlr3learners ships out with a dozen different popular learners.
We can list them from the its dictionary.
If we want more, we can load on extension package from mlr3's
[learner-org](https://github.com/mlr-org/mlr3learners/wiki) on GitHub.
Note how after the install the dictionary increases in size.

```{r}
head(as.data.table(mlr_learners)[, c("key", "packages")])
# remotes::install_github("mlr3learners/mlr3learners.randomForest")
library(mlr3learners.randomForest)
print(as.data.table(mlr_learners)[, c("key", "packages")])
```

## Benchmarking multiple learners

The `benchmark()` function can conveniently compare learners on the same dataset(s).

```{r}
learners = list(learner1, learner2, lrn("classif.randomForest"))
bm_grid = benchmark_grid(task, learners, cv10)
bm = benchmark(bm_grid)
print(bm)
print(bm$aggregate(measures = msrs(c("classif.acc", "classif.ce"))))
```

## Conclusion

We left out a lot of details and other features.
If you want to know more, read the [mlr3book](https://mlr3book.mlr-org.com/) and the documentation of the mentioned packages.

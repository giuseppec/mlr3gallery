---
title: mlr3 and OpenML - Moneyball use case
author: Philipp Kopper
date: '2020-03-21'
slug: mlr3-OpenML-Tuning
categories: []
tags: ['mlr3', 'OpenML', 'tuning']
packages: ['mlr3', 'mlr3learners', 'mlr3tuning']
---

This use case shows how to easily embed the rich data available via [OpenML](https://www.openml.org/) into our mlr3 workflow.
The idea is to show how to access a variety of real world data problems and how to tackle them via `mlr3`.
Furthermore, we show how a __good__ model can be found using `mlr3tuning`.
It assumes only little knowledge in ML or mlr3.
It requires no knowledge on OpenML or parameter tuning in mlr3.
Knowledge on missing value impuation is helpful but not necessary.
The [mlr3book](https://mlr3book.mlr-org.com/) is a detailed ressource of the methods and functions used here.

The following operations are shown:

* Creating tasks and learners
* Training and predicting
* Resampling / Cross-validation
* Tuning
* Benchmarking, to compare multiple learners (or specifications)

## Loading basic packages

```{r}
# tasks, train, predict, resample, benchmark
library("mlr3")
# about a dozen reasonable learners
library("mlr3learners")
# Tuning
library("mlr3tuning")
# lots of measures for evaluation
library("mlr3measures")
# Retreiving the data
library("OpenML")
```

## Retreiving the data from OpenML

Currently, we are still working on a generic embedding of `OpenML` into `mlr3`.
The use case will be updated as soon as we release the workflow.
However, we can still use the `OpenML` package to retrieve data (and more) straight away.
Typically, you can retrieve the data with an `id`.
The `id` can be found on [OpenML](https://www.openml.org/).
We choose the `41021`as our `oml_id`.
The related web page cann be accessed [here](https://www.openml.org/d/41021).
This data set is uploaded by [Joaquin Vanschoren](https://www.openml.org/u/2).

```{r}
oml_id = 41021

oml_dat = getOMLDataSet(data.id = oml_id)
```

The decription indicates that the data set is associated to baseball or more precisely the story of [Moneyball](https://www.imdb.com/title/tt1210166/).

```{r}
oml_dat
```

However, the description within the `OpenML` object is not very detailed.
The previously referenced [page](https://www.openml.org/d/41021) however states the following:

In the early 2000s, Billy Beane and Paul DePodesta worked for the Oakland Athletics. 
While there, they literally changed the game of baseball. 
They didn't do it using a bat or glove, and they certainly didn't do it by throwing money at the issue; in fact, money was the issue. 
They didn't have enough of it, but they were still expected to keep up with teams that had much deeper pockets. 
This is where Statistics came riding down the hillside on a white horse to save the day. 
This data set contains some of the information that was available to Beane and DePodesta in the early 2000s, and it can be used to better understand their methods.

This data set contains a set of variables that Beane and DePodesta focused heavily on. 
They determined that stats like on-base percentage (OBP) and slugging percentage (SLG) were very important when it came to scoring runs, however they were largely undervalued by most scouts at the time. 
This translated to a gold mine for Beane and DePodesta. 
Since these players weren't being looked at by other teams, they could recruit these players on a small budget. 
The variables are as follows:

* Team
* League
* Year
* Runs Scored (RS)
* Runs Allowed (RA)
* Wins (W)
* On-Base Percentage (OBP)
* Slugging Percentage (SLG)
* Batting Average (BA)
* Playoffs (binary)
* RankSeason
* RankPlayoffs
* Games Played (G)
* Opponent On-Base Percentage (OOBP)
* Opponent Slugging Percentage (OSLG)

While Beane and DePodesta defined most of these statistics and measures for individual players, this data set is on team level.

If you are a baseball enthusiast these statistics seem very informative.
If baseball of rather obscure to you, simply take these features as given or give this [article](https://en.wikipedia.org/wiki/Baseball_statistics) a quick view.

```{r}
data = oml_dat$data
summary(data)
```

The summary shows how this data we are dealing with looks like:
Some data is missing, however this has strucural reasons.
There are $39$ teams with each maximally $47$ years ($1962$ - $2012$) of observation.
For $988$ cases the information on `RankSeason` and `RankPlayoffs` is missing.
This is however because these simply did not reach the playoffs and hence have no reported rank.

```{r} 
summary(data[data$Playoffs == 0, c("RankSeason", "RankPlayoffs")])
```

On the other hand `OOBP` and `OSLG` have $812$ missing values.
It seems as if these measures are not available before $1998$.

```{r} 
summary(data[is.na(data$OOBP), "Year"])
```

We seem to have a missing data problem.
Typically in this case we have three options:
They are:

* Complete case analysis: Exclude all observation with missing values.

* Complete feature analysis: Exclude all features with missing values.

* Missing value imputation: Use a model to "guess" the missing values (based on the underlying distrubtion of the data.

Typically, missing value imputation is preferred over the first two.
However, in machine learning one can try out all options and see which performs best for the underlying problem.
For now we limit ourselves on a rather simple impuation technique, impuation by randomly sampling from the univariate distribution.
Note that this does not take the multvariate distribution into account properly and that there are more elaborate approaches.

`mlr3` has some solutions for that within the `mlr3pipelines` package.
We define the `PipeOp` in the following fashion:

```{r}
imp_missind = po("missind")
imp_num =  po("imputehist", param_vals = list(affect_columns = selector_type("numeric")))
imp_fct = po("imputenewlvl", param_vals = list(affect_columns = selector_type("factor")))
imputer = imp_missind %>>% imp_num
```

We only aim to impute `OOBP` and `OSLG`. 
For the other missing (categorical) features we use `PipeOpImputeNewlvl` to add a new level which indicates that information is missing.
So far we only prepared the pipeline without actually modeling.
Before we come to this step, we perform a last prepocessing step manually.
Namely, we make `G` (games played) a numerical feature.

```{r}
data$G = as.numeric(data$G)
```

Additionally, the row names (starting with $0$) are changed. 

```{r}
rownames(data) <- 1:nrow(data)
```

## Creating tasks and learners

The fact that there is missing data does not affect the `task` definition.
The `task` determines what is actually the probelm to be solved by machine learning.
We want to explain the Runs Scored (RS).
The task is defined by:

```{r}
# creates a mlr3 task from scratch, from a data.frame
# 'target' names the column in the dataset we want to learn to predict
task = TaskRegr$new(id = "moneyball", backend = data, target = "RS")
task$missings()
```

The `missings` method indicates what we already knew: our missing values.
Missing values are not always a problem. 
Some learners can deal with the pretty well.
However, today we want to use a random forest for our task.

```{r}
# creates a learner
test_lrn = LearnerRegrLM$new()
# displays the properties
test_lrn$properties
```

Typically, in `mlr3` the `properties` method would tell us whether missingness is a problem to this learner or not.
As it is not listed here, the random forest cannot deal with missingness.

The learner is the statistical approach to solve the problem implied by the `task` (and not only the model used).
As we aim to use imputation beforehand, we incorporate it into the learner.
The model we use within the learner is going to be a random forest from the `ranger` package.
One can allow the embedding of the preprocessing (imputation) into a learner by creating `new` (method) `PipeOpLearner` (R6 class).
This special `learner` can be put into a graph together with the `imputer` via the `new` method of the `GraphLearner` class.

```{r}
# creates a normal learner however allows further embedding of PipeOp's.
polrn = PipeOpLearner$new(mlr_learners$get("regr.lm"))
# sets number of trees to 1000
#polrn$param_set$values = list(num.trees = 1000)
# the final learner is a graph consisting of the imputer and the normal learner.
lrn = GraphLearner$new(graph = imputer %>>% polrn)
#
```

## Train and predict

To get a feeling how our model performs in the wild we simply train the `learner` on a subset of the data and predict on the held out data.

```{r}
# defines the training and testing data; 95% is used for training
train_set = sample(task$nrow, 0.95 * task$nrow)
test_set = setdiff(seq_len(task$nrow), train_set)
# train learner on subset of task
lrn$train(task, row_ids = train_set)
# predict using held out observations
preds = lrn$predict(task, row_ids = test_set)
print(preds)
```

## Evaluation

Let's score our prediction object with some metrics.
And take a deeper look by inspecting the confusion matrix.

```{r}
head(as.data.table(mlr_measures))
s = preds$score(msr("classif.acc"))
print(s)
s = preds$score(msrs(c("classif.acc", "classif.ce")))
print(s)
cm = preds$confusion
print(cm)
```

## Changing hyperpars

The learner contains information about all parameters that can be configured, including data type, constraints, defaults, etc.
We can change the hyperparameters either during construction of later through an active binding.

```{r}
print(learner1$param_set)
learner2 = lrn("classif.rpart", predict_type = "prob", minsplit = 50)
learner2$param_set$values$minsplit = 50
```

## Resampling

Resampling simply repeats the train-predict-score loop and collects all results in a nice `data.table`.

```{r, size = "tiny"}
cv10 = rsmp("cv", folds = 10)
r = resample(task, learner1, cv10)
print(r)
r$score(msrs(c("classif.acc", "classif.ce")))
print(r$data)
# get all predictions nicely concatenated in a table
preds = r$prediction()
print(preds)
cm = preds$confusion
print(cm)
```

## Populating the learner dictionary

mlr3learners ships out with a dozen different popular learners.
We can list them from the its dictionary.
If we want more, we can load on extension package from mlr3's
[learner-org](https://github.com/mlr-org/mlr3learners/wiki) on GitHub.
Note how after the install the dictionary increases in size.

```{r}
head(as.data.table(mlr_learners)[, c("key", "packages")])
# remotes::install_github("mlr3learners/mlr3learners.randomForest")
library(mlr3learners.randomForest)
print(as.data.table(mlr_learners)[, c("key", "packages")])
```

## Benchmarking multiple learners

The `benchmark()` function can conveniently compare learners on the same dataset(s).

```{r}
learners = list(learner1, learner2, lrn("classif.randomForest"))
bm_grid = benchmark_grid(task, learners, cv10)
bm = benchmark(bm_grid)
print(bm)
print(bm$aggregate(measures = msrs(c("classif.acc", "classif.ce"))))
```

## Conclusion

We left out a lot of details and other features.
If you want to know more, read the [mlr3book](https://mlr3book.mlr-org.com/) and the documentation of the mentioned packages.

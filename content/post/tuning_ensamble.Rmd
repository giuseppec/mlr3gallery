---
title: "Tuning a stacked learner "
author: "Milan Dragicevic"
date: '2020-03-31'
slug: tune-ensamble
categories: []
tags: ['stacking', 'multilevel stacking', 'ensemble tuning']
packages: ['mlr3', 'mlr3learners', 'mlr3pipelines', 'mlr3tuning', 'paradox']
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(mlr3)
library(mlr3learners)
library(mlr3pipelines)
library(mlr3tuning)
library(paradox)
```

## Intro 

Multilevel stacking is an ensemble technique where several models are trained on the data and then their predictions are added as new features along with the original features which are then used to train a new level of models. This can be repeated for several iterations until a final model is trained.  To limit over-fitting it is advisable not to use in-between learner predictions on the train sets but rather use test set (out-of-bag) predictions.   

In this post a multilevel stacking example will be described using [mlr3pipelines](https://mlr3pipelines.mlr-org.com/articles/introduction.html). A slightly different example is available in the [mlr3book](https://mlr3book.mlr-org.com/pipe-nonlinear.html#multilevel-stacking). An important distinction to the mentioned book example is in explaining how to tune the ensemble hyper parameters.  

In the following pipeline three learners: rpart, glmnet and lda will be trained on the input data in the first level. Prior to training rpart in the first level a custom selector is used which selects a random subset of columns (as an example), while prior to training glmnet and lda the data will be transformed using PCA. The test set predictions of these learners will then be concatenated with the original data and used as input to the second level learners. In the second level the data will first be transformed using PCA and an additional three learners : rpart, glmnet and lda will be trained. Finally the test set predictions from level 2 models will be concatenated with the level 2 input data and a ranger learner will be trained. The number of randomly selected columns and the number of principal components retained will be tuned jointly with other learner hyper parameters.

## Prerequisites

```{r packages}
library(mlr3)
library(mlr3learners)
library(mlr3pipelines)
library(mlr3tuning)
library(paradox)
```

To illustrate stacking the sonar task will be used:

```{r task}
sonar_task = tsk("sonar")
```


## Pipeline creation

### Level 1 learners

The level 1 learners will be created:

```{r lrn1}
rprt_lrn  = lrn("classif.rpart", predict_type = "prob")
glmnet_lrn =  lrn("classif.glmnet", predict_type = "prob")
lda_lrn = lrn("classif.lda", predict_type = "prob")
```

To use learner out-of-bag predictions [PipeOpLearnerCV()](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_learner_cv.html) will be used:  

```{r lrn2}
rprt_cv1 = po("learner_cv", rprt_lrn , id = "rprt_1")
glmnet_cv1 = po("learner_cv", glmnet_lrn, id = "glmnet_1")
lda_cv1 = po("learner_cv", lda_lrn, id = "lda_1")
```

Then the level 1 models will be defined and their predictions will be concatenated with the original data passed via [PipeOpNOP()](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_nop.html):  

```{r lrn3}
level1 = gunion(list(
  po("select", id = "sel1_1")  %>>%
    rprt_cv1,
  po("pca",
     id = "pca1_2",
     param_vals = list(scale. = TRUE)) %>>%
    glmnet_cv1,
  po("pca",
     id = "pca1_3",
     param_vals = list(scale. = TRUE)) %>>%
    lda_cv1,
  po("nop", id = "nop1")))  %>>%
  po("featureunion", id = "union1")
```

Check how it looks:  

```{r lrn4, fig.width=6, fig.height = 6}
level1$plot(html = FALSE)
```

### Level 2 learners

Next the level 2 models will be created:

```{r lrn5}
rprt_cv2 = po("learner_cv", rprt_lrn , id = "rprt_2")
glmnet_cv2 = po("learner_cv", glmnet_lrn, id = "glmnet_2")
lda_cv2 = po("learner_cv", lda_lrn, id = "lda_2")
```

And the second level of stacking will be defined where the level 2 model predictions will be concatenated with the original data along the level 1 model predictions. All level 2 models as input receive PCA transformed data:

```{r lrn6}
level2 = level1 %>>%
  po("copy", 4) %>>%
  gunion(list(
    po("pca",
       id = "pca2_1",
       param_vals = list(scale. = TRUE)) %>>%
      rprt_cv2,
    po("pca",
       id = "pca2_2",
       param_vals = list(scale. = TRUE)) %>>%
      glmnet_cv2,
    po("pca",
       id = "pca2_3",
       param_vals = list(scale. = TRUE))  %>>%
      lda_cv2,
    po("nop", id = "nop2")))  %>>%
  po("featureunion", id = "union2")
```

Create a final ranger learner:  

```{r lrn7}
ranger_lrn = lrn("classif.ranger")
ranger_lrn$predict_type <- "prob"
```

Make the ensemble and check how it looks:

```{r lrn8, fig.width=6, fig.height = 7}
ensamble = level2 %>>%
  ranger_lrn

ensamble$plot(html = FALSE)
```

### Defining the tuning space

In order to tune the ensemble a [paradox](https://paradox.mlr-org.com/) [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) will be created with a custom selector parameter `col_sample_ratio` and all the other learner hyper parameters we would like tuned. 

```{r ps1}
ps = ParamSet$new(
  list(
    ParamDbl$new("sel1_1.col_sample_ratio", 0.1, 1),
    ParamInt$new("pca1_2.rank.", 3, 50),
    ParamInt$new("pca1_3.rank.", 3, 20),
    ParamInt$new("pca2_1.rank.", 3, 50),
    ParamInt$new("pca2_2.rank.", 3, 50),
    ParamInt$new("pca2_3.rank.", 3, 20),
    ParamDbl$new("rprt_1.cp", 0.001, 0.1),
    ParamInt$new("rprt_1.minbucket", 1, 10),
    ParamDbl$new("glmnet_1.alpha", 0, 1),
    ParamDbl$new("rprt_2.cp", 0.001, 0.1),
    ParamInt$new("rprt_2.minbucket", 1, 10),
    ParamDbl$new("glmnet_2.alpha", 0, 1),
    ParamInt$new("classif.ranger.mtry", lower = 1L, upper = 10L),
    ParamDbl$new("classif.ranger.sample.fraction", lower = 0.5, upper = 1),
    ParamInt$new("classif.ranger.num.trees", lower = 5L, upper = 50L)
  ))
```

Next to define the `trafo` which will map `col_sample_ratio` to a custom selector function. To understand how this works check [this](https://mlr3gallery.mlr-org.com/select_uncorrelated_features/) post as well as the 
[mlr3book](https://mlr3book.mlr-org.com/paradox.html).

```{r ps2}
ps$trafo = function(x, param_set) {
  sel1_1.col_sample_ratio = x$sel1_1.col_sample_ratio
  x$sel1_1.col_sample_ratio = NULL
  x$sel1_1.selector = function(task) {
    fn = task$feature_names
    fn = sample(fn, sel1_1.col_sample_ratio * length(fn))
    fn
  }
  x
}
```

### Tuning 

Create a graph learner from the ensemble pipeline:

```{r ens1}
ens_lrn =  GraphLearner$new(ensamble)

ens_lrn$predict_type = "prob"
```

Define stratified resampling:

```{r ens2}
cv3 = rsmp("cv", folds = 3)

sonar_task$col_roles$stratum = sonar_task$target_names
```

As well as the tuner and the tuning instance:  

```{r ens3}
set.seed(123)
instance <- TuningInstance$new(
  task = sonar_task,
  learner = ens_lrn,
  resampling = cv3,
  measures = msr("classif.auc"),
  param_set = ps,
  terminator = term("evals", n_evals = 2) #to limit training time
)

tuner <- TunerRandomSearch$new()
```

And finally tune the ensemble. This takes a minute even with only 2 iterations of random search

```{r ens4}
tuner$tune(instance)

instance$archive()

instance$result$params
```

## Conclusion

This example shows the versatility of [mlr3pipelines](https://mlr3pipelines.mlr-org.com/articles/introduction.html).
By using more learners, varied representations of the data set as well as more levels, a powerful yet compute hungry pipeline can be created. It is important to note care should be taken to avoid name clashes of pipeline objects. In addition using random selection of features with just one learner with is probably not a good idea. 
